{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from Fundamentals of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K armed bandits\n",
    "Reinforcement learning trains based on evaluation of actions rather than from instructions.  Evlauative feedback indicates how good the action was, but not what the correct actoin to take is, while instructive feedback indicates what the correct action is. \n",
    "\n",
    "The $k$-armed bandit problem is one in which you face repeated choices, receiving a reward after each choice.  Each action has an expected (time invariant in this example) reward -- the value of the action.\n",
    "$$\n",
    "q_*(a) :=E \\,[R_t\\, \\vert \\, A_t=a]\n",
    "$$\n",
    "We do not know $q_*$, but estimate the value of action a at time $t$ with $Q_t(a)$.  At any given time step one action has highest expected value, and you choose between exploiting your current knowledge by choosing it (the greedy action) and exploring other actions (choosing them) in order to inprove your estimate of their value.\n",
    "\n",
    "Action-value methods:  methods for estimating action values and using the estimates to make decisions\n",
    "\n",
    "For example estimate expected reward using average reward (sample averaging):\n",
    "$$\n",
    "Q_t(a)=\\frac{\\textrm{sum of rewards when action a taken}}{\\textrm{number times action a taken}}  =\\frac{\\sum_{i=1}^{t-1} R_i 1_{A_i=a}}{\\sum_{i=1}^{t-1}1_{A_i=a}}\n",
    "$$\n",
    "Greedy selection is:\n",
    "$$\n",
    "A_t= \\textrm{argmax}_a Q_t(a)\n",
    "$$\n",
    "To induce exploration, you can choose epsilon greedy methods, in which you make the greedy choice most of the time, and sample randomly, independantly of estimated values, wiht probability epsilon.\n",
    "\n",
    "For averaging, the update rule is:\n",
    "$$\n",
    "Q_n=n^{-1} \\sum_{i=1}^n R_i \\,=\\, n^{-1} \\left( R_n + (n-1) Q_n \\right) \\, = \\, Q_n + n^{-1} \\left( R_n - Q_n \\right)\n",
    "$$\n",
    "$$\n",
    "NewEstimate=OldEstimate+StepSize\\, ( Target - oldEstimate)\n",
    "$$\n",
    "Where Target means the direction of travel.  \n",
    "For the nonstationary case, given more weight to recent rewards by discounting old rewards geometrically:\n",
    "$$\n",
    "Q_n + \\alpha \\left( R_n - Q_n \\right) =  \\sum_{i-1}^n \\alpha (1-\\alpha)^{n-i} R_i\n",
    "$$\n",
    "Sometime called an exponentially recency weighted average.  Alpha can be replaced by other sequence, in fact convergence will occur with probability 1 for any sequence such that\n",
    "$$\n",
    "\\sum_n \\alpha_n = \\infty \\quad \\sum_n \\alpha_n^2 < \\infty\n",
    "$$\n",
    "The first condition makes the steps big enough to overcome initial conditions and the second makes them small enough to assure convergence.  But convergence is not desired in a nonstationary environment.\n",
    "\n",
    "Value estimates are biased by initial conditions, and setting optimistic initial conditions encourages exploration early in the process.  Thus optimistic initial values tend to be good for stationary problems, but the temporary drive for exploration does not help in nonstationary problems.\n",
    "\n",
    "An alternative to using a fixed epsilon is to try to include a term to encourage exploration of infrequenly sampled states which represents the uncertainty of the estimate of actions values:\n",
    "$$\n",
    "A_t = \\textrm{argmax}_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}}  \\right]\n",
    "$$\n",
    "where $N_t(a)$ was the number of times a has been selected so far.\n",
    "\n",
    "Finally, instead of using value estimates, one could use the relative value of the actions,  selecting via softmax\n",
    "\n",
    "\n",
    "$$\n",
    "P(A_t=a)  = \\frac{e^{H_t(a)}}{e^{H_t(1)}+\\cdots + e^{H_t(k)} } :=\\pi_t(a)\n",
    "$$\n",
    "\n",
    "Where H is a preference function updated via stochastic gradient descent:\n",
    "\n",
    "$$\n",
    "H_{t+1}:=H_t+\\alpha(R_t-\\bar R_t) (1-\\pi_t)=H_t(a) -\\alpha(R_t-\\bar R_t)\\pi_t(a)\n",
    "$$\n",
    "Where $\\bar R_t$ is the mean of all rewards so far (baseline).\n",
    "\n",
    "In gradient descent, you minimize an objective function of the form \n",
    "$$\n",
    "F(v) = n^{-1} \\sum_{i=1}^n F_i(v) \n",
    "$$\n",
    "for a parameter v, with F being the error attributed to the ith observation.  This leads to an update process, with learning rate $\\eta$, consists of taking steps opposite the direction of hte gradient.  \n",
    "$$\n",
    "v\\leftarrow v-\\eta \\nabla F(v)\n",
    "$$\n",
    "In stochastic gradient descent, you choose one observation at a time time minimize with respect to:\n",
    "$$\n",
    "v\\leftarrow v-\\eta \\nabla F_i(v)\n",
    "$$\n",
    "In our example, the update step is \n",
    "$$\n",
    "H_{t+1}(a)=H_t(a) -\\alpha\\frac{\\partial E [R_t]}{\\partial H_t(a)} \\qquad E[R_t]=\\sum_x \\pi_t(x) q_*(x)\n",
    "$$\n",
    "Where x ranges over all actions.  We do not know $q_*$ Full Dirivation in Sutton on page 38.  \n",
    "Resources for real world application of contextual bandits problems:\n",
    "https://www.hunch.net/~rwil/ and https://vowpalwabbit.org/neurips2019/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision processes\n",
    "\n",
    "Finite MDP are used for associative evaluative feedback problems.  Actions influence immediate and delayed rewards, and values are state dependant $q_*(s,a)$.  The learner is the agent, which interacts with the environment.  The agent has a state, in which it takes an action, resulting in a reward and an updated state at a sequence of discrete time steps.  In a finite MDP the number of states, actions, and rewards are all finite (discrete).  \n",
    "$$\n",
    "p(s^\\prime, \\,r \\, \\vert \\, s,\\,a):=P(S_t={s^\\prime}, \\, R_t=r \\, \\vert \\, S_{t-1}=s,\\,A_{t-1}=a)\\qquad \\sum_s^\\prime \\sum_r p(s^\\prime, \\,r \\, \\vert \\, s,\\,a) =1\n",
    "$$\n",
    "The assumption that only the current state matters in an assumption that all relevant information about previous states is included in the current state.  Compute state transition probabilities by summing over rewards, and find expected rewareds by summing over states and averaging over rewards:\n",
    "$$\n",
    "r(s,a)=E[R_t\\, \\vert \\, S_{t-1}=s,\\, A_{t-1}=a]\\sum_r r \\sum_{s^\\prime} p(s^\\prime, \\,r \\, \\vert \\, s,\\,a) \\qquad \n",
    "r(s,a,s^\\prime)=E[R_t\\, \\vert \\, S_{t}=s^\\prime, \\,S_{t-1}=s, \\, A_{t-1}=a]\\sum_r r  p( \\,r \\, \\vert \\,s^\\prime,\\, s,\\,a)\n",
    "$$\n",
    "The actions represent choices made by the agent, the states represent the basis on which the choies are made, and rewards represent and interpretation of achieving goals.  Goals are formalized by rewards passing from environment to agent.  In RL goals and purposes are represented by the maximization of expected reward.  The reward signal does not impart prior knowledge of how to do a task, but only of what you want to achieve.  In general try to maximize future expected reward, expected return, denoted $G_t = F(R_{t+1}, \\dots, R_T)$ where T is a final time step.  Makes sense when rewards break down in terms of subsequences (episodes) that end in a terminal state.  Episodes begin independant of the previous episode.  Episodic tasks break down into episodes, while continuing tasks do not end.  In this case, future rewards may be discounted, and discouted return represented by \n",
    "$$\n",
    "G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} = R_{t+1}+\\gamma G_{t+1}\n",
    "$$\n",
    "We can develop conventions to express both continuing and episodic tasks by introducing absorbing states, in which all actions transistion to the same state and the reward is 0.  \n",
    "In order to plan out future actions, agents need to evaluate how good it is to be in a state -- not just the reward, but the reward you can expect in the future from going to that state.  Of course the value of future states will depend on what you plan to do once you get into a state, which we call a policy ($\\pi$).   This is the idea behind a value function v:\n",
    "$$\n",
    "v_\\pi(s) = E_\\pi[ G_t \\, \\vert \\, S_t=s ] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\vert \\, S_t=s \\right]\n",
    "$$\n",
    "the expected value of state s when you are following policy $\\pi$.  V is the state value function for policy pi.  We can devise a method to develop policies by calculating the value of a state action combination, assuming that I thereafter follow policy pi.  The action value function for policy pi is:\n",
    "$$\n",
    "q_\\pi(s,a)= E_\\pi[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\vert \\, S_t=s , \\, A_t=a \\right]\n",
    "$$\n",
    "One can estimate v and q by trying policies and take the average of the returns following a state (given an action), a process referred to as monte carlo methods because they estimate by averaging over many random samples of actual returns.  When there are too many states, one can use some number of parameters as a surrogate for the state.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation\n",
    "The state value Bellman equation is derived from v using the recursive nature of G, by separating the current state, in which we must choose an action, from the future, over which we have some sort of policy pi.\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = E_\\pi[ G_t \\, \\vert \\, S_t=s ] = E_\\pi \\left[ R_t + \\gamma G_{t+1} \\, \\vert \\, S_t=s \\right]\n",
    "$$\n",
    "Now we consider choosing and action a, which will lead us to a new state $s^\\prime$ with some reward r.\n",
    "$$\n",
    "\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma  E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1}=s^\\prime ] \\right)\n",
    "$$\n",
    "The terms involving summation come from calculating the probability of choosing action a and ending in state $s^\\prime$ with reward r, given that you are currently in state s.  The term in the expectation is then conditionally independent of a and s, given the new state $s^\\prime$.  Since we are assuming everything is stationary (time independent), v does not depend on time, only one state, so that:\n",
    "$$\n",
    "v_\\pi(s) =\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma  E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1}=s^\\prime ]\\right) =\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma   v_\\pi (s^\\prime) \\right)\n",
    "$$\n",
    "Similarly, for the state action function, \n",
    "$$\n",
    "q_\\pi(s,a)= E_\\pi[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ] = \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma   E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1} = s^\\prime ] \\right) \\\\ \n",
    "=\\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma \\sum_{a^\\prime} \\pi(a^\\prime | s^\\prime)  E_\\pi[ G_{t+1} \\, \\vert \\, A_{t+1}=a^\\prime, \\, S_{t+1}=s^\\prime ] \\right)\n",
    " =\\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma \\sum_{a^\\prime} \\pi(a^\\prime | s^\\prime)  q(s^\\prime,a^\\prime) \\right)\n",
    " $$\n",
    " The key observation is that the Bellman equation allows you to express the value of a state in terms of a linear combination of the values of the other states.  Thus you can solve a linear equation in order to get the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Policies\n",
    "Our goal is not just to evaluate states, but to find optimal policies to take in these states.  We compare two policies by saying one is better if and only if each state has a value at least as great.  An optimal oplicy is better than any other policy.\n",
    "$$\n",
    "\\pi_1\\leq \\pi_2 \\iff v_{\\pi_1}(s) \\leq v_{\\pi_2}(s) \\, \\forall s\n",
    "$$\n",
    "An optimal policy is denoted $\\pi_*$.  Similarly, we can define an optimal value for a state as the greatest value the state can have under any policy, which is its value under the optimal policy:\n",
    "$$\n",
    "v_{\\pi^*}(s) = E_{\\pi^*}[ G_t \\, \\vert \\, S_t=s ] = \\max v_{\\pi^*} (s) \\, \\forall s\n",
    "$$\n",
    "Similary, there is an optimal action-value function:\n",
    "$$\n",
    "q_*(s,a) =  q_{\\pi^*} (s,a) \\, \\forall \\,(s,a)\n",
    "$$\n",
    "which is related to $v_*$\n",
    "$$\n",
    "q_*(s,a) =  E_{\\pi^*}[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ]=E_{\\pi^*}[ R_t+\\gamma v_*(S_{t+1}) \\, \\vert \\, S_t=s, \\, A_t=a ]\n",
    "$$\n",
    "The Bellman optimality equations relate between states:\n",
    "$$\n",
    "v_{\\pi^*}(s) = \\max_a q_*(s,a) =\\max_a \\sum_{s^\\prime, r} p(s^\\prime, r \\, \\vert  \\, s,a)\\left[r+\\gamma v_*(s^\\prime) \\right]\n",
    "$$\n",
    "and state actions:\n",
    "$$\n",
    "q_*(s,a)= \\sum_{s^\\prime, r} p(s^\\prime, r \\, \\vert  \\, s,a)\\left[r+\\gamma \\max_{a^\\prime} q_*(s^\\prime,a^\\prime) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
