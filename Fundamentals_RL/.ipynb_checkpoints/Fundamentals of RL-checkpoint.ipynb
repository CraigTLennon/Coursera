{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from Fundamentals of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning trains based on evaluation of actions rather than from instructions.  Evlauative feedback indicates how good the action was, but not what the correct actoin to take is, while instructive feedback indicates what the correct action is. \n",
    "\n",
    "The $k$-armed bandit problem is one in which you face repeated choices, receiving a reward after each choice.  Each action has an expected (time invariant in this example) reward -- the value of the action.\n",
    "$$\n",
    "q_*(a) :=E \\,[R_t\\, \\vert \\, A_t=a]\n",
    "$$\n",
    "We do not know $q_*$, but estimate the value of action a at time $t$ with $Q_t(a)$.  At any given time step one action has highest expected value, and you choose between exploiting your current knowledge by choosing it (the greedy action) and exploring other actions (choosing them) in order to inprove your estimate of their value.\n",
    "\n",
    "Action-value methods:  methods for estimating action values and using the estimates to make decisions\n",
    "\n",
    "For example estimate expected reward using average reward (sample averaging):\n",
    "$$\n",
    "Q_t(a)=\\frac{\\textrm{sum of rewards when action a taken}}{\\textrm{number times action a taken}}  =\\frac{\\sum_{i=1}^{t-1} R_i 1_{A_i=a}}{\\sum_{i=1}^{t-1}1_{A_i=a}}\n",
    "$$\n",
    "Greedy selection is:\n",
    "$$\n",
    "A_t= \\textrm{argmax}_a Q_t(a)\n",
    "$$\n",
    "To induce exploration, you can choose epsilon greedy methods, in which you make the greedy choice most of the time, and sample randomly, independantly of estimated values, wiht probability epsilon.\n",
    "\n",
    "For averaging, the update rule is:\n",
    "$$\n",
    "Q_n=n^{-1} \\sum_{i=1}^n R_i \\,=\\, n^{-1} \\left( R_n + (n-1) Q_n \\right) \\, = \\, Q_n + n^{-1} \\left( R_n - Q_n \\right)\n",
    "$$\n",
    "$$\n",
    "NewEstimate=OldEstimate+StepSize\\, ( Target - oldEstimate)\n",
    "$$\n",
    "Where Target means the direction of travel.  \n",
    "For the nonstationary case, given more weight to recent rewards by discounting old rewards geometrically:\n",
    "$$\n",
    "Q_n + \\alpha \\left( R_n - Q_n \\right) =  \\sum_{i-1}^n \\alpha (1-\\alpha)^{n-i} R_i\n",
    "$$\n",
    "Sometime called an exponentially recency weighted average.  Alpha can be replaced by other sequence, in fact convergence will occur with probability 1 for any sequence such that\n",
    "$$\n",
    "\\sum_n \\alpha_n = \\infty \\quad \\sum_n \\alpha_n^2 < \\infty\n",
    "$$\n",
    "The first condition makes the steps big enough to overcome initial conditions and the second makes them small enough to assure convergence.  But convergence is not desired in a nonstationary environment.\n",
    "\n",
    "Value estimates are biased by initial conditions, and setting optimistic initial conditions encourages exploration early in the process.  Thus optimistic initial values tend to be good for stationary problems, but the temporary drive for exploration does not help in nonstationary problems.\n",
    "\n",
    "An alternative to using a fixed epsilon is to try to include a term to encourage exploration of infrequenly sampled states which represents the uncertainty of the estimate of actions values:\n",
    "$$\n",
    "A_t = \\textrm{argmax}_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}}  \\right]\n",
    "$$\n",
    "where $N_t(a)$ was the number of times a has been selected so far.\n",
    "\n",
    "Finally, instead of using value estimates, one could use the relative value of the actions,  selecting via softmax\n",
    "$$\n",
    "P(A_t=a) = \\frac{e^{H_t(a)}{e^{H_t(1)+\\cdots + e^{H_t(k)}:=\\pi_t(a)\n",
    "$$\n",
    "\n",
    "Where H is a preference function updated via stochastic gradient descent:\n",
    "\n",
    "$$\n",
    "H_{t+1}:=H_t+\\alpha(R_t-\\bar R_t) (1-\\pi_t)=H_t(a) -\\alpha(R_t-\\bar R_t)\\pi_t(a)\n",
    "$$\n",
    "Where $\\bar R_t$ is the mean of all rewards so far (baseline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
