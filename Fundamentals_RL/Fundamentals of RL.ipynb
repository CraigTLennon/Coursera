{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes from Fundamentals of RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K armed bandits\n",
    "Reinforcement learning trains based on evaluation of actions rather than from instructions.  Evlauative feedback indicates how good the action was, but not what the correct actoin to take is, while instructive feedback indicates what the correct action is. \n",
    "\n",
    "The $k$-armed bandit problem is one in which you face repeated choices, receiving a reward after each choice.  Each action has an expected (time invariant in this example) reward -- the value of the action.\n",
    "$$\n",
    "q_*(a) :=E \\,[R_t\\, \\vert \\, A_t=a]\n",
    "$$\n",
    "We do not know $q_*$, but estimate the value of action a at time $t$ with $Q_t(a)$.  At any given time step one action has highest expected value, and you choose between exploiting your current knowledge by choosing it (the greedy action) and exploring other actions (choosing them) in order to inprove your estimate of their value.\n",
    "\n",
    "Action-value methods:  methods for estimating action values and using the estimates to make decisions\n",
    "\n",
    "For example estimate expected reward using average reward (sample averaging):\n",
    "$$\n",
    "Q_t(a)=\\frac{\\textrm{sum of rewards when action a taken}}{\\textrm{number times action a taken}}  =\\frac{\\sum_{i=1}^{t-1} R_i 1_{A_i=a}}{\\sum_{i=1}^{t-1}1_{A_i=a}}\n",
    "$$\n",
    "Greedy selection is:\n",
    "$$\n",
    "A_t= \\textrm{argmax}_a Q_t(a)\n",
    "$$\n",
    "To induce exploration, you can choose epsilon greedy methods, in which you make the greedy choice most of the time, and sample randomly, independantly of estimated values, wiht probability epsilon.\n",
    "\n",
    "For averaging, the update rule is:\n",
    "$$\n",
    "Q_n=n^{-1} \\sum_{i=1}^n R_i \\,=\\, n^{-1} \\left( R_n + (n-1) Q_n \\right) \\, = \\, Q_n + n^{-1} \\left( R_n - Q_n \\right)\n",
    "$$\n",
    "$$\n",
    "NewEstimate=OldEstimate+StepSize\\, ( Target - oldEstimate)\n",
    "$$\n",
    "Where Target means the direction of travel.  \n",
    "For the nonstationary case, given more weight to recent rewards by discounting old rewards geometrically:\n",
    "$$\n",
    "Q_n + \\alpha \\left( R_n - Q_n \\right) =  \\sum_{i-1}^n \\alpha (1-\\alpha)^{n-i} R_i\n",
    "$$\n",
    "Sometime called an exponentially recency weighted average.  Alpha can be replaced by other sequence, in fact convergence will occur with probability 1 for any sequence such that\n",
    "$$\n",
    "\\sum_n \\alpha_n = \\infty \\quad \\sum_n \\alpha_n^2 < \\infty\n",
    "$$\n",
    "The first condition makes the steps big enough to overcome initial conditions and the second makes them small enough to assure convergence.  But convergence is not desired in a nonstationary environment.\n",
    "\n",
    "Value estimates are biased by initial conditions, and setting optimistic initial conditions encourages exploration early in the process.  Thus optimistic initial values tend to be good for stationary problems, but the temporary drive for exploration does not help in nonstationary problems.\n",
    "\n",
    "An alternative to using a fixed epsilon is to try to include a term to encourage exploration of infrequenly sampled states which represents the uncertainty of the estimate of actions values:\n",
    "$$\n",
    "A_t = \\textrm{argmax}_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}}  \\right]\n",
    "$$\n",
    "where $N_t(a)$ was the number of times a has been selected so far.\n",
    "\n",
    "Finally, instead of using value estimates, one could use the relative value of the actions,  selecting via softmax\n",
    "\n",
    "\n",
    "$$\n",
    "P(A_t=a)  = \\frac{e^{H_t(a)}}{e^{H_t(1)}+\\cdots + e^{H_t(k)} } :=\\pi_t(a)\n",
    "$$\n",
    "\n",
    "Where H is a preference function updated via stochastic gradient descent:\n",
    "\n",
    "$$\n",
    "H_{t+1}:=H_t+\\alpha(R_t-\\bar R_t) (1-\\pi_t)=H_t(a) -\\alpha(R_t-\\bar R_t)\\pi_t(a)\n",
    "$$\n",
    "Where $\\bar R_t$ is the mean of all rewards so far (baseline).\n",
    "\n",
    "In gradient descent, you minimize an objective function of the form \n",
    "$$\n",
    "F(v) = n^{-1} \\sum_{i=1}^n F_i(v) \n",
    "$$\n",
    "for a parameter v, with F being the error attributed to the ith observation.  This leads to an update process, with learning rate $\\eta$, consists of taking steps opposite the direction of hte gradient.  \n",
    "$$\n",
    "v\\leftarrow v-\\eta \\nabla F(v)\n",
    "$$\n",
    "In stochastic gradient descent, you choose one observation at a time time minimize with respect to:\n",
    "$$\n",
    "v\\leftarrow v-\\eta \\nabla F_i(v)\n",
    "$$\n",
    "In our example, the update step is \n",
    "$$\n",
    "H_{t+1}(a)=H_t(a) -\\alpha\\frac{\\partial E [R_t]}{\\partial H_t(a)} \\qquad E[R_t]=\\sum_x \\pi_t(x) q_*(x)\n",
    "$$\n",
    "Where x ranges over all actions.  We do not know $q_*$ Full Dirivation in Sutton on page 38.  \n",
    "Resources for real world application of contextual bandits problems:\n",
    "https://www.hunch.net/~rwil/ and https://vowpalwabbit.org/neurips2019/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision processes\n",
    "\n",
    "Finite MDP are used for associative evaluative feedback problems.  Actions influence immediate and delayed rewards, and values are state dependant $q_*(s,a)$.  The learner is the agent, which interacts with the environment.  The agent has a state, in which it takes an action, resulting in a reward and an updated state at a sequence of discrete time steps.  In a finite MDP the number of states, actions, and rewards are all finite (discrete).  \n",
    "$$\n",
    "p(s^\\prime, \\,r \\, \\vert \\, s,\\,a):=P(S_t={s^\\prime}, \\, R_t=r \\, \\vert \\, S_{t-1}=s,\\,A_{t-1}=a)\\qquad \\sum_s^\\prime \\sum_r p(s^\\prime, \\,r \\, \\vert \\, s,\\,a) =1\n",
    "$$\n",
    "The assumption that only the current state matters in an assumption that all relevant information about previous states is included in the current state.  Compute state transition probabilities by summing over rewards, and find expected rewareds by summing over states and averaging over rewards:\n",
    "$$\n",
    "r(s,a)=E[R_t\\, \\vert \\, S_{t-1}=s,\\, A_{t-1}=a]\\sum_r r \\sum_{s^\\prime} p(s^\\prime, \\,r \\, \\vert \\, s,\\,a) \\qquad \n",
    "r(s,a,s^\\prime)=E[R_t\\, \\vert \\, S_{t}=s^\\prime, \\,S_{t-1}=s, \\, A_{t-1}=a]\\sum_r r  p( \\,r \\, \\vert \\,s^\\prime,\\, s,\\,a)\n",
    "$$\n",
    "The actions represent choices made by the agent, the states represent the basis on which the choies are made, and rewards represent and interpretation of achieving goals.  Goals are formalized by rewards passing from environment to agent.  In RL goals and purposes are represented by the maximization of expected reward.  The reward signal does not impart prior knowledge of how to do a task, but only of what you want to achieve.  In general try to maximize future expected reward, expected return, denoted $G_t = F(R_{t+1}, \\dots, R_T)$ where T is a final time step.  Makes sense when rewards break down in terms of subsequences (episodes) that end in a terminal state.  Episodes begin independant of the previous episode.  Episodic tasks break down into episodes, while continuing tasks do not end.  In this case, future rewards may be discounted, and discouted return represented by \n",
    "$$\n",
    "G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} = R_{t+1}+\\gamma G_{t+1}\n",
    "$$\n",
    "We can develop conventions to express both continuing and episodic tasks by introducing absorbing states, in which all actions transistion to the same state and the reward is 0.  \n",
    "In order to plan out future actions, agents need to evaluate how good it is to be in a state -- not just the reward, but the reward you can expect in the future from going to that state.  Of course the value of future states will depend on what you plan to do once you get into a state, which we call a policy ($\\pi$).   This is the idea behind a value function v:\n",
    "$$\n",
    "v_\\pi(s) = E_\\pi[ G_t \\, \\vert \\, S_t=s ] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\vert \\, S_t=s \\right]\n",
    "$$\n",
    "the expected value of state s when you are following policy $\\pi$.  V is the state value function for policy pi.  We can devise a method to develop policies by calculating the value of a state action combination, assuming that I thereafter follow policy pi.  The action value function for policy pi is:\n",
    "$$\n",
    "q_\\pi(s,a)= E_\\pi[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\vert \\, S_t=s , \\, A_t=a \\right]\n",
    "$$\n",
    "One can estimate v and q by trying policies and take the average of the returns following a state (given an action), a process referred to as monte carlo methods because they estimate by averaging over many random samples of actual returns.  When there are too many states, one can use some number of parameters as a surrogate for the state.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation\n",
    "The state value Bellman equation is derived from v using the recursive nature of G, by separating the current state, in which we must choose an action, from the future, over which we have some sort of policy pi.\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = E_\\pi[ G_t \\, \\vert \\, S_t=s ] = E_\\pi \\left[ R_t + \\gamma G_{t+1} \\, \\vert \\, S_t=s \\right]\n",
    "$$\n",
    "Now we consider choosing and action a, which will lead us to a new state $s^\\prime$ with some reward r.\n",
    "$$\n",
    "\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma  E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1}=s^\\prime ] \\right)\n",
    "$$\n",
    "The terms involving summation come from calculating the probability of choosing action a and ending in state $s^\\prime$ with reward r, given that you are currently in state s.  The term in the expectation is then conditionally independent of a and s, given the new state $s^\\prime$.  Since we are assuming everything is stationary (time independent), v does not depend on time, only one state, so that:\n",
    "$$\n",
    "v_\\pi(s) =\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma  E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1}=s^\\prime ]\\right) =\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma   v_\\pi (s^\\prime) \\right)\n",
    "$$\n",
    "Similarly, for the state action function, \n",
    "$$\n",
    "q_\\pi(s,a)= E_\\pi[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ] = \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma   E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1} = s^\\prime ] \\right) \\\\ \n",
    "=\\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma \\sum_{a^\\prime} \\pi(a^\\prime | s^\\prime)  E_\\pi[ G_{t+1} \\, \\vert \\, A_{t+1}=a^\\prime, \\, S_{t+1}=s^\\prime ] \\right)\n",
    " =\\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma \\sum_{a^\\prime} \\pi(a^\\prime | s^\\prime)  q(s^\\prime,a^\\prime) \\right)\n",
    " $$\n",
    " The key observation is that the Bellman equation allows you to express the value of a state in terms of a linear combination of the values of the other states.  Thus you can solve a linear equation in order to get the values.  With a finite number of states, there is a uniques solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Policies\n",
    "Our goal is not just to evaluate states, but to find optimal policies to take in these states.  We compare two policies by saying one is better if and only if each state has a value at least as great.  An optimal oplicy is better than any other policy.\n",
    "$$\n",
    "\\pi_1\\leq \\pi_2 \\iff v_{\\pi_1}(s) \\leq v_{\\pi_2}(s) \\, \\forall s\n",
    "$$\n",
    "An optimal policy is denoted $\\pi_*$.  Similarly, we can define an optimal value for a state as the greatest value the state can have under any policy, which is its value under the optimal policy:\n",
    "$$\n",
    "v_{\\pi^*}(s) = E_{\\pi^*}[ G_t \\, \\vert \\, S_t=s ] = \\max v_{\\pi^*} (s) \\, \\forall s\n",
    "$$\n",
    "Similary, there is an optimal action-value function:\n",
    "$$\n",
    "q_*(s,a) =  q_{\\pi^*} (s,a) \\, \\forall \\,(s,a)\n",
    "$$\n",
    "which is related to $v_*$\n",
    "$$\n",
    "q_*(s,a) =  E_{\\pi^*}[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ]=E_{\\pi^*}[ R_t+\\gamma v_*(S_{t+1}) \\, \\vert \\, S_t=s, \\, A_t=a ]\n",
    "$$\n",
    "The Bellman optimality equations relate between states:\n",
    "$$\n",
    "v_{\\pi^*}(s) = \\max_a q_*(s,a) =\\max_a \\sum_{s^\\prime, r} p(s^\\prime, r \\, \\vert  \\, s,a)\\left[r+\\gamma v_*(s^\\prime) \\right]\n",
    "$$\n",
    "and state actions:\n",
    "$$\n",
    "q_*(s,a)= \\sum_{s^\\prime, r} p(s^\\prime, r \\, \\vert  \\, s,a)\\left[r+\\gamma \\max_{a^\\prime} q_*(s^\\prime,a^\\prime) \\right]\n",
    "$$\n",
    "If you have $v_*$ there will be some actions for which the maximum value is obtained, and you can choose an optimal policy by assigning non-zero probabilities only to those actions.  To find these actions you only need a one step search, so a greedy seach works if you have $v_*$.  And if you have $q_*$, you just choose any action that maximizes it.  While it would seem like this is the best way to solve RL problems, the solution relies on 3 things, understanding environmental dynamics, computational resources for finding a solution, and the markov property.  Usually at least one of these properties is violated.  With few enough actions and states you can make a table of optimal choices, but with a large number of states, you must represent choices as parameterized functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Programming\n",
    "DP is a collection of algorithms, relying on a great environmental model and computational availability, which can find solutions to an MDP.  Policy evaluation, or prediction, computes $v_\\pi$.  One method of so doing is through incremental update, in which you calculate a new state value given old state values and the known transitions and rewards, which are taken as fixed.  Starting with the bellman equation:\n",
    "$$\n",
    "v_\\pi = \\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma   v_\\pi (s^\\prime) \\right)\n",
    "$$\n",
    "we update it, for any fixed $\\pi$ as\n",
    "$$\n",
    "v_{k+1}= \\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma   v_k (s^\\prime) \\right)\n",
    "$$\n",
    "and eventually it will converge to $v_\\pi$.  This algorithm is called iterative policy evaluation.  \n",
    "\n",
    "Supposing we have found the value function for some policy $\\pi$.  Then we can use this evaluation to improve on the policy by examining each state s and making a better choice.  This is the policy improvement theorem, that if, for all s, \n",
    "$q(s,\\pi_\\prime(s))\\geq v_\\pi(s)$ then the prime policy is better than the original policy.  Thus we develop a new greedy policy by making a greedy choice with respect to the original policy. For each state s, let \n",
    "$$\n",
    "\\pi^\\prime(s) = \\text{argmax}_a q_\\pi(s,a) \n",
    "$$ \n",
    "This policy is better than the original policy, or equal, in which case the policy is optimal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration and Value Iteration\n",
    "\n",
    "Having improved the policy, one can then recompute values, and use this in turn to make a new policy, which is called policy iteration.  Alternatively, one can improve both the policy and the estimate of the optimal policy in the same iteration.\n",
    "\n",
    "$$\n",
    "v_{k+1}(s)= \\max_a \\sum_{s^\\prime, r} p(s^\\prime, r\\, \\vert \\, s,\\, a) \\left[r+ \\gamma v_k(s^\\prime) \\right]\n",
    "$$\n",
    "This both selects a greedy action with respect to the current policy and updates the value of the current state in one step.  The policy greedy with respect to the valuation to which the sequence of v converge is the optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample based learning\n",
    "Monte Carlo methods require only experience, samples of states, actions, and rewards from the environment, and not a complete knowledge of the environment required for dynamic programming.  The model for the environment needs only to generate transitions, and not complete distributions.  By monte carlo we mean methods based on averaging complete returns, not partial returns, which will be considered later.  We use general policy iteration, in which we learn value functions and improve policies iteratively.  For every visit MC, start with a policy pi, choose arbitrary initial state values, and prepare empty lists for each state to store the return for that state.  Then for each episode, generate a sequence of actions, states, and rewards by following the policy.  Use the rewards to calculate a discounted value for each time step, and append that value to the list of returns for that state, and average over rewards for the state to get state values.  Then repeat with a new episode.  Alternatively, append only the first visit to the state to the returns list, (first return mc) (p 92).\n",
    "\n",
    "One advantage of mc methods is that the computation required to estimate the value of one state does not depend on the total number of states.\n",
    "\n",
    "Without a model of the environment, knowledge of state values alone woll not determine a policy, so knowledge of state action pairs of required.  The application of the mc methods described above, with a deterministic policy pi, will result in limited exploration.  In order to maintain esploration, we could specify a starting state action pair, after executing which we follow pi.  This is called exploring starts.  Alternativily we could consider policies which are stochastic.  \n",
    "\n",
    "Having looked at the prediction problem, above, we can consider mc control.  Here we again follow generalized policy iteration, as in the dynamic policy case.  For example, with mc exploring starts, we start with an arbitrary policy pi and an arbitrary value function for each state action pair, as well as an empty list for each state-action pair.  Then for as many episodes as we want, choose and initial starting state and action pair randomly, and, starting from that pair, follow the policy pi.  looping backwards, record that value of each state action pair.  Append that value to the list for that state action pair, and obtain a new estimate of the value of the pair by averaging over all values in the list.  Let your new policy be the greedy policy based on those state action pairs.  Then repeat for another episode.\n",
    "\n",
    "#### $\\epsilon$ greedy and soft policies\n",
    "\n",
    "As with the k armed bandit problem, we can keep exploring, through a combination of on policy and off policy methods.  On policy methods attempt to improve the policy used to make decisions, and off policy methods attempt to evaluate or improve a policy different from the one generating the other data.  For the on policy methods, you keep $\\pi(a \\vert s)>0\\, \\forall a,s$ but gradually move closer to a deterministic policy.  In the case of $\\epsilon $ greedy algorithms, you choose a non-greedy policy with probability $\\epsilon / \\vert A(s)\\vert$ where A is the number of actions in state s, and otherwise choose the greedy action.  This choice happens in the policy update phase, when a new policy is chosen.\n",
    "\n",
    "For off policy learning, you have two policies, the target policy and the behavior policy.  In the prediction problem, we start with a target policy $\\pi$ and a behavior policy b, with the assumption that $\\pi(a\\vert s)>0 \\rightarrow b(a\\vert s)>0 $.  Since we sample from b and not the target policy, we need to make adjustments to the results of the sample by a technique called importance sampling.\n",
    "\n",
    "#### Importance sampling\n",
    "The challenge of off policy learning is that we sample from a different distribution than we want to estimate.  We sample from $x \\tilde b$, which we could use to estimate $E_b [X]$, and we want $E_\\pi[X]$, so we have to adjust for this sampling.  The good thing is that we know both b and pi.  This leads to importance sampling, and the importance sampling ratio.   \n",
    "$$\n",
    "E_\\pi[X]=\\sum_x x \\pi(x) =\\sum_x x\\frac{\\pi(x)}{b(x)} b(x) =\\sum_x x\\rho(x) b(x) \n",
    "$$\n",
    "where rho is the sampling ratio.  From a given starting state $S_t$, the probability of a trajectory is\n",
    "$$\n",
    "\\Pi_{k=1}^{T-1} \\pi(a_k|s_k)p(s_{k+1}|a_k)\n",
    "$$\n",
    "and the importance sampling ratio is \n",
    "$$\n",
    "\\rho_{t:T-1}= \\Pi_{k=1}^{T-1} \\frac{\\pi(a_k|s_k)p(s_{k+1}|a_k)}{b(a_k|s_k)p(s_{k+1}|a_k)}, \n",
    "$$\n",
    "so that $E_b[\\rho_{t:T-1}G_t | S_t=s] =v_\\pi(s)$.  The algorithm for off policy mc contol for estimating Q begins with arbitrary values for state action values and a cumulative weight for each state action pair.  Then for each episode, choose a soft policy b, and generate an action state reward sequence using b, initialing rho to 1.  For each step, update the reward.  Update the cumulative weight by adding rho, and then update the state action value by adding the product of rho over the cumulative weight and the difference of the reward and the current estimated value.  Then update rho to the next t value.  \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
