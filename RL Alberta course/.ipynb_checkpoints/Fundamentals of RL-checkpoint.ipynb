{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K armed bandits\n",
    "Reinforcement learning trains based on evaluation of actions rather than from instructions.  Evlauative feedback indicates how good the action was, but not what the correct actoin to take is, while instructive feedback indicates what the correct action is. \n",
    "\n",
    "The $k$-armed bandit problem is one in which you face repeated choices, receiving a reward after each choice.  Each action has an expected (time invariant in this example) reward -- the value of the action.\n",
    "$$\n",
    "q_*(a) :=E \\,[R_t\\, \\vert \\, A_t=a]\n",
    "$$\n",
    "We do not know $q_*$, but estimate the value of action a at time $t$ with $Q_t(a)$.  At any given time step one action has highest expected value, and you choose between exploiting your current knowledge by choosing it (the greedy action) and exploring other actions (choosing them) in order to inprove your estimate of their value.\n",
    "\n",
    "Action-value methods:  methods for estimating action values and using the estimates to make decisions\n",
    "\n",
    "For example estimate expected reward using average reward (sample averaging):\n",
    "$$\n",
    "Q_t(a)=\\frac{\\textrm{sum of rewards when action a taken}}{\\textrm{number times action a taken}}  =\\frac{\\sum_{i=1}^{t-1} R_i 1_{A_i=a}}{\\sum_{i=1}^{t-1}1_{A_i=a}}\n",
    "$$\n",
    "Greedy selection is:\n",
    "$$\n",
    "A_t= \\textrm{argmax}_a Q_t(a)\n",
    "$$\n",
    "To induce exploration, you can choose epsilon greedy methods, in which you make the greedy choice most of the time, and sample randomly, independantly of estimated values, wiht probability epsilon.\n",
    "\n",
    "For averaging, the update rule is:\n",
    "$$\n",
    "Q_n=n^{-1} \\sum_{i=1}^n R_i \\,=\\, n^{-1} \\left( R_n + (n-1) Q_n \\right) \\, = \\, Q_n + n^{-1} \\left( R_n - Q_n \\right)\n",
    "$$\n",
    "$$\n",
    "NewEstimate=OldEstimate+StepSize\\, ( Target - oldEstimate)\n",
    "$$\n",
    "Where Target means the direction of travel.  \n",
    "For the nonstationary case, given more weight to recent rewards by discounting old rewards geometrically:\n",
    "$$\n",
    "Q_n + \\alpha \\left( R_n - Q_n \\right) =  \\sum_{i-1}^n \\alpha (1-\\alpha)^{n-i} R_i\n",
    "$$\n",
    "Sometime called an exponentially recency weighted average.  Alpha can be replaced by other sequence, in fact convergence will occur with probability 1 for any sequence such that\n",
    "$$\n",
    "\\sum_n \\alpha_n = \\infty \\quad \\sum_n \\alpha_n^2 < \\infty\n",
    "$$\n",
    "The first condition makes the steps big enough to overcome initial conditions and the second makes them small enough to assure convergence.  But convergence is not desired in a nonstationary environment.\n",
    "\n",
    "Value estimates are biased by initial conditions, and setting optimistic initial conditions encourages exploration early in the process.  Thus optimistic initial values tend to be good for stationary problems, but the temporary drive for exploration does not help in nonstationary problems.\n",
    "\n",
    "An alternative to using a fixed epsilon is to try to include a term to encourage exploration of infrequenly sampled states which represents the uncertainty of the estimate of actions values:\n",
    "$$\n",
    "A_t = \\textrm{argmax}_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}}  \\right]\n",
    "$$\n",
    "where $N_t(a)$ was the number of times a has been selected so far.\n",
    "\n",
    "Finally, instead of using value estimates, one could use the relative value of the actions,  selecting via softmax\n",
    "\n",
    "\n",
    "$$\n",
    "P(A_t=a)  = \\frac{e^{H_t(a)}}{e^{H_t(1)}+\\cdots + e^{H_t(k)} } :=\\pi_t(a)\n",
    "$$\n",
    "\n",
    "Where H is a preference function updated via stochastic gradient descent:\n",
    "\n",
    "$$\n",
    "H_{t+1}:=H_t+\\alpha(R_t-\\bar R_t) (1-\\pi_t)=H_t(a) -\\alpha(R_t-\\bar R_t)\\pi_t(a)\n",
    "$$\n",
    "Where $\\bar R_t$ is the mean of all rewards so far (baseline).\n",
    "\n",
    "In gradient descent, you minimize an objective function of the form \n",
    "$$\n",
    "F(v) = n^{-1} \\sum_{i=1}^n F_i(v) \n",
    "$$\n",
    "for a parameter v, with F being the error attributed to the ith observation.  This leads to an update process, with learning rate $\\eta$, consists of taking steps opposite the direction of hte gradient.  \n",
    "$$\n",
    "v\\leftarrow v-\\eta \\nabla F(v)\n",
    "$$\n",
    "In stochastic gradient descent, you choose one observation at a time time minimize with respect to:\n",
    "$$\n",
    "v\\leftarrow v-\\eta \\nabla F_i(v)\n",
    "$$\n",
    "In our example, the update step is \n",
    "$$\n",
    "H_{t+1}(a)=H_t(a) -\\alpha\\frac{\\partial E [R_t]}{\\partial H_t(a)} \\qquad E[R_t]=\\sum_x \\pi_t(x) q_*(x)\n",
    "$$\n",
    "Where x ranges over all actions.  We do not know $q_*$ Full Dirivation in Sutton on page 38.  \n",
    "Resources for real world application of contextual bandits problems:\n",
    "https://www.hunch.net/~rwil/ and https://vowpalwabbit.org/neurips2019/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov decision processes\n",
    "\n",
    "Finite MDP are used for associative evaluative feedback problems.  Actions influence immediate and delayed rewards, and values are state dependant $q_*(s,a)$.  The learner is the agent, which interacts with the environment.  The agent has a state, in which it takes an action, resulting in a reward and an updated state at a sequence of discrete time steps.  In a finite MDP the number of states, actions, and rewards are all finite (discrete).  \n",
    "$$\n",
    "p(s^\\prime, \\,r \\, \\vert \\, s,\\,a):=P(S_t={s^\\prime}, \\, R_t=r \\, \\vert \\, S_{t-1}=s,\\,A_{t-1}=a)\\qquad \\sum_s^\\prime \\sum_r p(s^\\prime, \\,r \\, \\vert \\, s,\\,a) =1\n",
    "$$\n",
    "The assumption that only the current state matters in an assumption that all relevant information about previous states is included in the current state.  Compute state transition probabilities by summing over rewards, and find expected rewareds by summing over states and averaging over rewards:\n",
    "$$\n",
    "r(s,a)=E[R_t\\, \\vert \\, S_{t-1}=s,\\, A_{t-1}=a]\\sum_r r \\sum_{s^\\prime} p(s^\\prime, \\,r \\, \\vert \\, s,\\,a) \\qquad \n",
    "r(s,a,s^\\prime)=E[R_t\\, \\vert \\, S_{t}=s^\\prime, \\,S_{t-1}=s, \\, A_{t-1}=a]\\sum_r r  p( \\,r \\, \\vert \\,s^\\prime,\\, s,\\,a)\n",
    "$$\n",
    "The actions represent choices made by the agent, the states represent the basis on which the choies are made, and rewards represent and interpretation of achieving goals.  Goals are formalized by rewards passing from environment to agent.  In RL goals and purposes are represented by the maximization of expected reward.  The reward signal does not impart prior knowledge of how to do a task, but only of what you want to achieve.  In general try to maximize future expected reward, expected return, denoted $G_t = F(R_{t+1}, \\dots, R_T)$ where T is a final time step.  Makes sense when rewards break down in terms of subsequences (episodes) that end in a terminal state.  Episodes begin independant of the previous episode.  Episodic tasks break down into episodes, while continuing tasks do not end.  In this case, future rewards may be discounted, and discouted return represented by \n",
    "$$\n",
    "G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} = R_{t+1}+\\gamma G_{t+1}\n",
    "$$\n",
    "We can develop conventions to express both continuing and episodic tasks by introducing absorbing states, in which all actions transistion to the same state and the reward is 0.  \n",
    "In order to plan out future actions, agents need to evaluate how good it is to be in a state -- not just the reward, but the reward you can expect in the future from going to that state.  Of course the value of future states will depend on what you plan to do once you get into a state, which we call a policy ($\\pi$).   This is the idea behind a value function v:\n",
    "$$\n",
    "v_\\pi(s) = E_\\pi[ G_t \\, \\vert \\, S_t=s ] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\vert \\, S_t=s \\right]\n",
    "$$\n",
    "the expected value of state s when you are following policy $\\pi$.  V is the state value function for policy pi.  We can devise a method to develop policies by calculating the value of a state action combination, assuming that I thereafter follow policy pi.  The action value function for policy pi is:\n",
    "$$\n",
    "q_\\pi(s,a)= E_\\pi[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ] = E_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\, \\vert \\, S_t=s , \\, A_t=a \\right]\n",
    "$$\n",
    "One can estimate v and q by trying policies and take the average of the returns following a state (given an action), a process referred to as monte carlo methods because they estimate by averaging over many random samples of actual returns.  When there are too many states, one can use some number of parameters as a surrogate for the state.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Equation\n",
    "The state value Bellman equation is derived from v using the recursive nature of G, by separating the current state, in which we must choose an action, from the future, over which we have some sort of policy pi.\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = E_\\pi[ G_t \\, \\vert \\, S_t=s ] = E_\\pi \\left[ R_t + \\gamma G_{t+1} \\, \\vert \\, S_t=s \\right]\n",
    "$$\n",
    "Now we consider choosing and action a, which will lead us to a new state $s^\\prime$ with some reward r.\n",
    "$$\n",
    "\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma  E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1}=s^\\prime ] \\right)\n",
    "$$\n",
    "The terms involving summation come from calculating the probability of choosing action a and ending in state $s^\\prime$ with reward r, given that you are currently in state s.  The term in the expectation is then conditionally independent of a and s, given the new state $s^\\prime$.  Since we are assuming everything is stationary (time independent), v does not depend on time, only one state, so that:\n",
    "$$\n",
    "v_\\pi(s) =\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma  E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1}=s^\\prime ]\\right) =\\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma   v_\\pi (s^\\prime) \\right)\n",
    "$$\n",
    "Similarly, for the state action function, \n",
    "$$\n",
    "q_\\pi(s,a)= E_\\pi[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ] = \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma   E_\\pi[ G_{t+1} \\, \\vert \\, S_{t+1} = s^\\prime ] \\right) \\\\ \n",
    "=\\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma \\sum_{a^\\prime} \\pi(a^\\prime | s^\\prime)  E_\\pi[ G_{t+1} \\, \\vert \\, A_{t+1}=a^\\prime, \\, S_{t+1}=s^\\prime ] \\right)\n",
    " =\\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a)\\left(r+\\gamma \\sum_{a^\\prime} \\pi(a^\\prime | s^\\prime)  q(s^\\prime,a^\\prime) \\right)\n",
    " $$\n",
    " The key observation is that the Bellman equation allows you to express the value of a state in terms of a linear combination of the values of the other states.  Thus you can solve a linear equation in order to get the values.  With a finite number of states, there is a uniques solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Policies\n",
    "Our goal is not just to evaluate states, but to find optimal policies to take in these states.  We compare two policies by saying one is better if and only if each state has a value at least as great.  An optimal oplicy is better than any other policy.\n",
    "$$\n",
    "\\pi_1\\leq \\pi_2 \\iff v_{\\pi_1}(s) \\leq v_{\\pi_2}(s) \\, \\forall s\n",
    "$$\n",
    "An optimal policy is denoted $\\pi_*$.  Similarly, we can define an optimal value for a state as the greatest value the state can have under any policy, which is its value under the optimal policy:\n",
    "$$\n",
    "v_{\\pi^*}(s) = E_{\\pi^*}[ G_t \\, \\vert \\, S_t=s ] = \\max v_{\\pi^*} (s) \\, \\forall s\n",
    "$$\n",
    "Similary, there is an optimal action-value function:\n",
    "$$\n",
    "q_*(s,a) =  q_{\\pi^*} (s,a) \\, \\forall \\,(s,a)\n",
    "$$\n",
    "which is related to $v_*$\n",
    "$$\n",
    "q_*(s,a) =  E_{\\pi^*}[ G_t \\, \\vert \\, S_t=s, \\, A_t=a ]=E_{\\pi^*}[ R_t+\\gamma v_*(S_{t+1}) \\, \\vert \\, S_t=s, \\, A_t=a ]\n",
    "$$\n",
    "The Bellman optimality equations relate between states:\n",
    "$$\n",
    "v_{\\pi^*}(s) = \\max_a q_*(s,a) =\\max_a \\sum_{s^\\prime, r} p(s^\\prime, r \\, \\vert  \\, s,a)\\left[r+\\gamma v_*(s^\\prime) \\right]\n",
    "$$\n",
    "and state actions:\n",
    "$$\n",
    "q_*(s,a)= \\sum_{s^\\prime, r} p(s^\\prime, r \\, \\vert  \\, s,a)\\left[r+\\gamma \\max_{a^\\prime} q_*(s^\\prime,a^\\prime) \\right]\n",
    "$$\n",
    "If you have $v_*$ there will be some actions for which the maximum value is obtained, and you can choose an optimal policy by assigning non-zero probabilities only to those actions.  To find these actions you only need a one step search, so a greedy seach works if you have $v_*$.  And if you have $q_*$, you just choose any action that maximizes it.  While it would seem like this is the best way to solve RL problems, the solution relies on 3 things, understanding environmental dynamics, computational resources for finding a solution, and the markov property.  Usually at least one of these properties is violated.  With few enough actions and states you can make a table of optimal choices, but with a large number of states, you must represent choices as parameterized functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Programming\n",
    "DP is a collection of algorithms, relying on a great environmental model and computational availability, which can find solutions to an MDP.  Policy evaluation, or prediction, computes $v_\\pi$.  One method of so doing is through incremental update, in which you calculate a new state value given old state values and the known transitions and rewards, which are taken as fixed.  Starting with the bellman equation:\n",
    "$$\n",
    "v_\\pi = \\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma   v_\\pi (s^\\prime) \\right)\n",
    "$$\n",
    "we update it, for any fixed $\\pi$ as\n",
    "$$\n",
    "v_{k+1}= \\sum_a \\pi(a|s) \\sum_{s^\\prime} \\sum_{r} p(s^\\prime,r | s,a) \\left(r+\\gamma   v_k (s^\\prime) \\right)\n",
    "$$\n",
    "and eventually it will converge to $v_\\pi$.  This algorithm is called iterative policy evaluation.  \n",
    "\n",
    "Supposing we have found the value function for some policy $\\pi$.  Then we can use this evaluation to improve on the policy by examining each state s and making a better choice.  This is the policy improvement theorem, that if, for all s, \n",
    "$q(s,\\pi_\\prime(s))\\geq v_\\pi(s)$ then the prime policy is better than the original policy.  Thus we develop a new greedy policy by making a greedy choice with respect to the original policy. For each state s, let \n",
    "$$\n",
    "\\pi^\\prime(s) = \\text{argmax}_a q_\\pi(s,a) \n",
    "$$ \n",
    "This policy is better than the original policy, or equal, in which case the policy is optimal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Iteration and Value Iteration\n",
    "\n",
    "Having improved the policy, one can then recompute values, and use this in turn to make a new policy, which is called policy iteration.  Alternatively, one can improve both the policy and the estimate of the optimal policy in the same iteration.\n",
    "\n",
    "$$\n",
    "v_{k+1}(s)= \\max_a \\sum_{s^\\prime, r} p(s^\\prime, r\\, \\vert \\, s,\\, a) \\left[r+ \\gamma v_k(s^\\prime) \\right]\n",
    "$$\n",
    "This both selects a greedy action with respect to the current policy and updates the value of the current state in one step.  The policy greedy with respect to the valuation to which the sequence of v converge is the optimal policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample based learning\n",
    "Monte Carlo methods require only experience, samples of states, actions, and rewards from the environment, and not a complete knowledge of the environment required for dynamic programming.  The model for the environment needs only to generate transitions, and not complete distributions.  By monte carlo we mean methods based on averaging complete returns, not partial returns, which will be considered later.  We use general policy iteration, in which we learn value functions and improve policies iteratively.  For every visit MC, start with a policy pi, choose arbitrary initial state values, and prepare empty lists for each state to store the return for that state.  Then for each episode, generate a sequence of actions, states, and rewards by following the policy.  Use the rewards to calculate a discounted value for each time step, and append that value to the list of returns for that state, and average over rewards for the state to get state values.  Then repeat with a new episode.  Alternatively, append only the first visit to the state to the returns list, (first return mc) (p 92).\n",
    "\n",
    "One advantage of mc methods is that the computation required to estimate the value of one state does not depend on the total number of states.\n",
    "\n",
    "Without a model of the environment, knowledge of state values alone woll not determine a policy, so knowledge of state action pairs of required.  The application of the mc methods described above, with a deterministic policy pi, will result in limited exploration.  In order to maintain esploration, we could specify a starting state action pair, after executing which we follow pi.  This is called exploring starts.  Alternativily we could consider policies which are stochastic.  \n",
    "\n",
    "Having looked at the prediction problem, above, we can consider mc control.  Here we again follow generalized policy iteration, as in the dynamic policy case.  For example, with mc exploring starts, we start with an arbitrary policy pi and an arbitrary value function for each state action pair, as well as an empty list for each state-action pair.  Then for as many episodes as we want, choose and initial starting state and action pair randomly, and, starting from that pair, follow the policy pi.  looping backwards, record that value of each state action pair.  Append that value to the list for that state action pair, and obtain a new estimate of the value of the pair by averaging over all values in the list.  Let your new policy be the greedy policy based on those state action pairs.  Then repeat for another episode.\n",
    "\n",
    "#### $\\epsilon$ greedy and soft policies\n",
    "\n",
    "As with the k armed bandit problem, we can keep exploring, through a combination of on policy and off policy methods.  On policy methods attempt to improve the policy used to make decisions, and off policy methods attempt to evaluate or improve a policy different from the one generating the other data.  For the on policy methods, you keep $\\pi(a \\vert s)>0\\, \\forall a,s$ but gradually move closer to a deterministic policy.  In the case of $\\epsilon $ greedy algorithms, you choose a non-greedy policy with probability $\\epsilon / \\vert A(s)\\vert$ where A is the number of actions in state s, and otherwise choose the greedy action.  This choice happens in the policy update phase, when a new policy is chosen.\n",
    "\n",
    "For off policy learning, you have two policies, the target policy and the behavior policy.  In the prediction problem, we start with a target policy $\\pi$ and a behavior policy b, with the assumption that $\\pi(a\\vert s)>0 \\rightarrow b(a\\vert s)>0 $.  Since we sample from b and not the target policy, we need to make adjustments to the results of the sample by a technique called importance sampling.\n",
    "\n",
    "#### Importance sampling\n",
    "The challenge of off policy learning is that we sample from a different distribution than we want to estimate.  We sample from $x \\sim b$, which we could use to estimate $E_b [X]$, and we want $E_\\pi[X]$, so we have to adjust for this sampling.  The good thing is that we know both b and pi.  This leads to importance sampling, and the importance sampling ratio.   \n",
    "$$\n",
    "E_\\pi[X]=\\sum_x x \\pi(x) =\\sum_x x\\frac{\\pi(x)}{b(x)} b(x) =\\sum_x x\\rho(x) b(x) \n",
    "$$\n",
    "where rho is the sampling ratio.  From a given starting state $S_t$, the probability of a trajectory is\n",
    "$$\n",
    "\\Pi_{k=1}^{T-1} \\pi(a_k|s_k)p(s_{k+1}|a_k)\n",
    "$$\n",
    "and the importance sampling ratio is \n",
    "$$\n",
    "\\rho_{t:T-1}= \\Pi_{k=1}^{T-1} \\frac{\\pi(a_k|s_k)p(s_{k+1}|a_k)}{b(a_k|s_k)p(s_{k+1}|a_k)}, \n",
    "$$\n",
    "so that $E_b[\\rho_{t:T-1}G_t | S_t=s] =v_\\pi(s)$.  The algorithm for off policy mc contol for estimating Q begins with arbitrary values for state action values and a cumulative weight for each state action pair.  Then for each episode, choose a soft policy b, and generate an action state reward sequence using b, initialing rho to 1.  For each step, update the reward.  Update the cumulative weight by adding rho, and then update the state action value by adding the product of rho over the cumulative weight and the difference of the reward and the current estimated value.  Then update rho to the next t value.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Difference Learning\n",
    "TD Learning learns without a model of the environment, like MC learning, but updates based on previous updates like DP, and without waiting for an outcome to the episode.  For control, DP, MC, and TD all use generalized policy iteration, alternating between prediction for a given policy and policy optimization, and differ primarily in their approach to prediction.  Compare a simple version of MC with a simple version of TD.\n",
    "Simple MC estimates state value after each espisode based on the returns from that state:\n",
    "$$\n",
    "v(s_t) \\leftarrow v(s_t) +  \\alpha \\left[ G_t -v(s_t) \\right]\n",
    "$$\n",
    "while TD(0) makes the update after each action (in the next state)\n",
    "$$\n",
    "v(s_t) \\leftarrow v(s_t) +  \\alpha \\left[R_t + \\gamma v(s_{t+1}) -v(s_t) \\right] \\approx v(s_t) +  \\alpha \\left[R_t + \\gamma G_{t+1} -v(s_t) \\right]\n",
    "$$\n",
    "The TD(0) algorithm starts with a policy pi, arbitrary state values, a fixed step size alpha.  For each episode, randomly initialize the start state, and for each step of the episode, taken an action according to the policy and observe the reward and new state.  Then update the state value for the current state according to the equation above.  \n",
    "\n",
    "Since TD updates based on its present estimates, like DP, it is referred to as a bootstrapping method.  \n",
    "MC uses an estimate of \n",
    "$$\n",
    "v_\\pi(s) = E [G_t\\, | \\, S_t=s]\n",
    "$$\n",
    "as a target, with the expectation  part unknown because there is no model of the environment.\n",
    "DP uses \n",
    "$$\n",
    "v_\\pi(s) = E [G_t\\, | \\, S_t=s]= E [R_t + \\gamma v_\\pi(s_{t+1})\\, | \\, S_t=s]\n",
    "$$\n",
    "as a target, and is estimating by using $V(S_{t+1})$ instead of $v_\\pi$.  TD estimates on both counts, using a current state estimate like DP and an estimation of the expectation.\n",
    "TD and MC both use sample updates, meaning they look ahead to the value of the next state to compute a new present value.  By contrast, expected updates, like in DP, are based on a distribution over all possible successors. \n",
    "TD error is the difference between the estimated value of $S_t$ and teh updated estimate provided by $R_t + \\gamma V(S_{t+1})$, and is denoted\n",
    "$$\n",
    "\\delta_t = R_{t+1} + \\gamma V(S_{t+1})-V(S_t)\n",
    "$$\n",
    "With this definition, and assuming that the array V is not updated between time t and time T, MC errors can be written as sums of deltas.\n",
    "$$\n",
    "G_t-V(S_t) = \\sum_{k=1}^{T-1} \\gamma^{k-1} \\delta_k\n",
    "$$\n",
    "In the instance in which only a finite amount of experience is available, a common approach is to repeatedly present the experience, updating the value functions once the experience has been presented.   This is batch updating.\n",
    "Under batch updating, MC will converge to state values that are sample averages of the actual returns after visiting each state s.  These are optimal in that they minimize the mean squated error from the actual returns in hte training set. (p172)  However, if we make some assumptions, for example involving the process being markov, the we have more information available then just the training data, and may make a better estimate with, say, TD.  Batch MC methods estimate in order to minimize the mean squared error on the training set, and TD(0) methods estimates to find the maximum liklihood model of the markov process.  The MLE estimate is the parameter value estimate whose probability of generating the data is greatest.  Given a markov model, we can estimate the value function as if the model were exactly correct, which is the certainty-equivalence estimate because it is equivalent to assuming that hte estimate of hte underlying process was known with certainty.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA \n",
    "\n",
    "Sarsa is about using TD prediction methods for control, which involves generalized policy iteration with TD methods for the precition aspect.  As with mc methods, we can choose on or off policy.  We lean an action value rather than a state value function $q=q_\\pi(a,s)$, following the same update rule as with the value state functions\n",
    "$$\n",
    "q(S_t,A_t) \\leftarrow q(S_t,A_t) + \\alpha \\left[ R_t +\\gamma q(S_{t+1},A_{t+1})- q(S_t,A_t)\\right] \n",
    "$$\n",
    "updating after every transition.  Since this uses S,A,R,S,A for the new and old states and actions, it is called sarsa.  For on policy sarsa, begin by initializing q and choosing a policy pi, perhaps epsilon greedy.  Loop over episodes, and for each step of each loop, take an action A, observing R and the next state, then select the action according to the policy, and update q as in the equation above.  \n",
    "\n",
    "#### Q Learning \n",
    "Q learning is the off policy version of TD control, intended to approximate the optimal policy.  The update is based on a greedy choice of action according to the current learned state action function.  \n",
    "$$\n",
    "q(S_t,A_t) \\leftarrow q(S_t,A_t) + \\alpha \\left[ R_t +\\gamma \\max_a q(S_{t+1},A_{t+1})- q(S_t,A_t)\\right]\n",
    "$$\n",
    "The new value is learned based on the optimal policy, with the chosen policy determining what state-action pairs are visited.  With this modification, the off policy Q learning follows the same pattern as sarsa.  \n",
    "#### Differences\n",
    " Q Learning only makes changes when it learns one action is better than another, while sarsa takes into account each action it takes.  For example, in eps greedy policies, sarsa would learn from effects of the random choices, while q learning would not, unless the random choice is better.  \n",
    " \n",
    " #### Expected SARSA\n",
    " \n",
    " Expected sarsa, instead of sampling one action, takes an expectation over possible actions, with the result that the estimates are more stable (less variance), but the computational burden is greater.  The difference between this an Q learning is in the update step.  \n",
    "$$\n",
    "q(S_t,A_t) \\leftarrow q(S_t,A_t) + \\alpha \\left[ R_t +\\gamma \\sum_a \\pi(a|s) q(s,a)- q(S_t,A_t)\\right]\n",
    "$$\n",
    "\n",
    "#### N step TD (bootstrapping)\n",
    "MC methods perform updating at the end of an episode, which is some, perhaps unknown number of steps (if the episode ends), while TD as described above performs an update after every step.  As a compromise, one could conduct updates after some fixed number of steps, or after an infinite number (at the end of the eisode) as in MC.  To do this, you chose some number of time steps (has random been considered?) n, and calculate an update at time t at time $t+n$.  The value of a state is calculated using the next n rewards:  \n",
    "$$\n",
    "G_{t:t+n}:= R_{t+1}+ \\gamma R_{t+2} + \\cdots + \\gamma^{n-1}R_{t+n}+ \\gamma^{n}V_{t+n-1}(S_{t+n})\n",
    "$$\n",
    "with the udate proceeding as\n",
    "$$\n",
    "V_{t+n}(S_{t})\\leftarrow V_{t+n-1}(S_{t}) + \\alpha \\left[G_{t:t+n}-   V_{t+n-1}(S_{t})\\right]\n",
    "$$\n",
    "Likewise, for control you can use n step sarsa:\n",
    "$$\n",
    "G_{t:t+n}:= R_{t+1}+ \\gamma R_{t+2} + \\cdots + \\gamma^{n-1}R_{t+n}+ \\gamma^{n}Q_{t+n-1}(S_{t+n},A_{t+n})\n",
    "$$\n",
    "with the udate proceeding as\n",
    "$$\n",
    "Q_{t+n}(S_{t},A_{t})\\leftarrow Q_{t+n-1}(S_{t},A_{t}) + \\alpha \\left[G_{t:t+n}-   Q_{t+n-1}(S_{t},A_{t})\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models and tabular methods\n",
    "Some RL methods, like dynamic programming and heuristic search, rely on a model of the environment, while others, like TD and MC, do not.  This distinction is captured in the separation between model-based and model-free reinforcement learning.  Model based methods already know their environment, so focus on planning, while model free methods do not, and focus on learning.  Both are based on value function computation, and both look ahead to future events, compute back up values, and then use it as an update target for and approximate value funtion.  \n",
    "\n",
    "A model of the environment is anything the agent uses to predict how the environment will respond to its actions.  In a stochastic environment, this could be a distribution model, which describes all possibles and probabilities, and sample models, which produce just one (or more) possibilities.  Dyanmic programming, with its conditional expectations, is a distribution model.  The MC models are sample models.  Sample models are compact, but incomplete, and would miss unlikely events, good or bad (is this why humans imagine both great and horrible events).  \n",
    " \n",
    " Models can be used to simulate experience.  A sample model can generate an episode, and a distribution model can generate all possible episodes.  The model is used to simulate the environment, and produce a simulated experience.  \n",
    " \n",
    " Planning is any computational process which takes a model as input and produces a policy for interacting with the modeled environment.  State space planning is a search through the state space for an optimal policy, and actions cause transitions between states, with value functions being computed over states.  Alternatively, in plan space planning, you search through the space of plans, with operators transforming one plan into another.\n",
    " The idea here is that state space models share a common structure:  all state space planning methods compute value functions to improve policy, and do so by updates applied to simulated experience.  So models produce simulated experience, from which updates produce values, which are used to produce policies.  In the model free case, real experience, through updates, produces values, which are used to produce policies.\n",
    " \n",
    "#### Dyna-Q\n",
    "Dyna is a example architecture demonstrating the functions of online planning.  For a planning agent, real experience both improves the model, and directly improves the value function and policies.  The former is model learning, and the latter is direct reinforcement learning.  Great pic on page 184.  When the agent learns via simulation through the model, it is indirect reinforcement learning.  Indirect methods make better use of available information, while direct methods are simpler and not affected by biases in the design of the model.  For the Dyna agent, planning, acting, model-learning and direct RL occur simultaneously and in parallel.  \n",
    "\n",
    "Tabular Dyna-Q works as follows:\n",
    "Intialize the state action value function Q and the Model state action function M(s,a) which predicts the next state and reward for state action pair s,a (whereas Q is an estimate of the value given a policy).  Then, in a loop, start in some state S, and choose A according to some policy like epsilon greedy whatever.  Take action A and observe reward R and the new state s'.  Update Q as in the Q learning algorithm, and then update the model with new reward and next action (assuming a deterministic environment).  Now learn indirectly, by repeating this loop n times:  1) choose a random S,A from among states previously experienced and actions previously taken, 2) Assign rewards and next states as recorded in the model, 3) update Q as in Q learning.  \n",
    "The implication is that actual experience happens on some time frame longer than the time it takes to learn from the model. \n",
    "\n",
    "In the case in which the environment is changing, exploration of alternative states can be encouraged by adding a simulated bonus reward to states based on how long ago they were explored.  One suggested reward is a constant multiplied by the square root of the time since the state was last visited.  \n",
    "##### Quadratic approximation\n",
    "Drew mentions that he often uses a quadratic model approximation for control in ground vehicle domain and suggests RL paper \"Model-based RL in Contextual Decision Processes: PAC bounds and Exponential Improvements over Model-free Approaches\" & Modern adaptive control and reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate solution methods\n",
    "For many problems, the state space is too large for a table to hold all of it, much less for the agent to actually explore all of it.  Thus functions interpolating between known states can be used.  We generalize between the states we have observed with function approximation.  Because is takes observations from the value function and attempts to generalize from them to construct the value function.  It is an instance of supervised learning dealing with a particular type of data issues, like non stationarity, bootstrapping and delayed targets.  \n",
    "\n",
    "#### On policy prediction\n",
    "The approximate value function depends of weights, w, so the approximate value of state s given weights w and policy pi, is \n",
    "$\\hat v(s,w) \\approx v_\\pi$. The number of weights is far fewer than the number of states, and changing one weight changes many states which is the generalization part.  Parameterization also helps with partially observable problems, because parameterization without depending on aspects of a state allows that parameterization to work for unobervable aspects.  \n",
    "\n",
    "The perspective so far has been that we are making an update to an estimated value function by ehifting the value at a particular state toward an update target for that state, $s \\rightarrow u$.  For MC, this is $S_t \\rightarrow G_t$, for TD(0) it is $S_t \\rightarrow R_t+ \\gamma \\hat v (S_{t+1})$, for TD(n) it is $S_t \\rightarrow G_{t:t+n}$, and for dynamic programming it is $S_t \\rightarrow E \\left[ R_t+\\gamma \\hat v(S_{t+1}) \\vert S_t=s \\right] $  with function approximation, the update will shift the value of many states at once.\n",
    "\n",
    "With more states than weights, better estimates for some states will lead to worse estimates for others, so we specify both a weighting of states and a measure of error in the model.  The weighting is the state diestribution, and our first error measure is mean squared value error.  State distribution\n",
    "$$\n",
    "\\sum_s \\mu(s) =1 \\qquad \\mu \\geq 0;  \\qquad VE(s):= \\sum_s \\mu(s) \\left[ v_\\pi(s) - \\hat v(s,\\vec w) \\right]\n",
    "$$\n",
    "Often mu is set to be the fraction of time spent in a state.  For episodic tasks, determine the expected amount of time spent in a state by adding the probability of starting in a state (h) to the probability of transitioning to that state from another, given the expected amount of time spent in the other state.  With $\\eta$ as the expected time spent in a state, \n",
    "$$\n",
    "\\eta(s) =h(s) + \\sum_x \\eta(x) \\sum_a \\pi( a \\vert, x) p(s \\vert a,x)\n",
    "$$\n",
    "Then get mu by normalizing eta.  The system of equations can be solved for eta.  Discounting can be included in the summation.\n",
    "With the error measure chosen, we turn to finding a global minimum by finding $w^*$ that minimizes our error.\n",
    "\n",
    "#### Gradient descent methods\n",
    "Start with a weights vector of length d and an approximate value function of the state s depending on those weights.\n",
    "$\\vec w =(w_1,\\cdots. w_d)^T$ (column vector) and $\\hat v (s, \\vec w)$.  The value of the weight vector at time t is $w_t$.  Supposing we observe a new value for a state at time t $v_\\pi(S_t)$.  Even though we have the exact correct value, we want to only make a small change in the direction of that value, because changing the value of one state changes the values of many states.  Under that assumption that states appear in the same distribution as we are trying to minimize, we can update by minimizing error following each observation (sample), which is stochastic gradient descent.\n",
    "$$\n",
    "\\vec w_{t+1} \\leftarrow \\vec w_{t} -  \\frac{1}{2}\\alpha \\nabla \\left[ v_\\pi (S_t)  - \\hat v(S_t,\\vec w)  \\right]^2 = \\vec w_{t} -  \\alpha  \\left[ v_\\pi (S_t)  - \\hat v(S_t,\\vec w)  \\right] \\nabla \\hat v(S_t,\\vec w) \n",
    "$$\n",
    "Above, alpha is a positive step size parameter, the error is a scalar, and the gradient is a vector containing the partial derivative of $ \\hat v$ with respect to each weight term.  Convergence assumes that alpha decreases over time.  With this ideal case set out, we now look at the more realitic case in which we approximate the true value instead of having it.\n",
    "\n",
    "Suppose that the target output for state s at time t is $U_t \\approx v_\\pi (S_t)$.  We still perform the target estimate as above, with U in place of $v_\\pi$, and it U is unbiased (if $E \\left[ U_t \\, \\vert \\, S_t=s  \\right] = v_\\pi (s)$ then w will converge to a local optimum.  For example, the MC target $U_t=G_t$ is unbiased, so gradient descent MC will find a local optima.  Just (202) loop over teh episode and use the returns G to do an update for each step in the episode.\n",
    "\n",
    "If a bootstrapping estimate is used for U the estimate will be biased, and so does not have convergence guarantees.  In the minimization update equation, the form assumes that the true value is independent of the weights, which is not the case in bootstrapping.  Since bootstrapping takes into account the weight value effects on the approximation function, but ignore the effect on the target, they are not gradient descent, but semi-gradient methods.  They do not converge as robustly, but can converge much faster.  They also allow for online learning.  In semi-gradient TD(0), the update is \n",
    "$$\n",
    "w \\leftarrow w + \\alpha \\left[ R+ \\gamma \\hat v (s_{new},w) -\\hat v (s,w) \\right] \\nabla \\hat v (s,w)\n",
    "$$\n",
    "\n",
    "A simple generalizing funtion is state aggregation, in which states are grouped together with one estimated value for each group.\n",
    "\n",
    "#### Linear methods\n",
    "\n",
    "A simple instance of function approximation is when the approximate function $\\hat v $ is a linear function of the weights $w$.  In this case, for every state there is a vector of features $\\vec x= (x_1(s) , \\cdots, x_d(s))^T$ with as many weights as features.  The state value is $$\n",
    "\\hat v (s, \\vec w) = \\vec w^T x(s) = \\sum_{i=1}^d w_i x_i(s)\n",
    "$$ where above both w and x are column vectors.  For linear methods, the features (the image of $x:S\\rightarrow R^d$ are called basis functions because they for a linear basis for the set of approximate functions (205).  Selecting a set d of basis functions is the same as constructing d dim features vectors to represent states.\n",
    "\n",
    "For SGD, $\\nabla \\hat v (s, w) = x(s)$, so the update rule is\n",
    "$$\n",
    "w_{t+1}  = w_t + \\alpha \\left[U_t-\\hat v (S_t,w_t) \\right]x(S_t)\n",
    "$$\n",
    "For semi-gradient TD(0), the weights also converge, but as it is biased, they may not converge to a local optima.  The update step is \n",
    "$$\n",
    "w_{t+1}  = w_t + \\alpha \\left[R_{t+1}+ \\gamma \\hat v (S_{t+1},w_t) -\\hat v (S_t,w_t) \\right]x_t\n",
    "= w_t + \\alpha \\left[R_{t+1}+ \\gamma w_t^T x_{t+1}  -w_t^T x_{t} \\right]x_t\n",
    "$$\n",
    "Since you can multiply a vector on either side by a scalar, the line above can be rearranged as \n",
    "$$\n",
    "w_{t+1} = w_t + \\alpha \\left[R_{t+1}x_t -x_t(w_t^T x_{t}- \\gamma w_t^T x_{t+1}  ) \\right] w_t + \\alpha \\left[R_{t+1}x_t -x_t(x_{t}- \\gamma x_{t+1}  ) w_t^T  \\right] = w_t +\\alpha(b_t-A_t w_t)\n",
    "$$\n",
    "where b captures the reward term and the matrix A is the outer product of the column vectors involving x.  At convergence, A and b no longer depend on t, and \n",
    "$$\n",
    "E [w_{t+1} | w_t] = w_t +E [R_{t+1}x_t]+E [x_t(x_{t}- \\gamma x_{t+1}  ) ]=w_t +\\alpha(b-Aw_t) = (I- \\alpha A) w_t+\\alpha b\n",
    "$$\n",
    "and it converges to $w^*$ satisfying $w^* = A^{-1}b$ as long as the matrix term multiplied by w is positive definite.\n",
    "\n",
    "##### Finding features\n",
    "Methods of course coding involve breaking up the feature space into connected regions that overlap, like circles or tiles, with the overlap allowing for discrimination and the area covered by the shape allowing for generalization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Networks for RL\n",
    "In another notebook\n",
    "\n",
    "#### Least Squares TD\n",
    "This method aims at directly computing the TD fixed point $w^*=A^{-1}b$ iterative estimates of A and b.  This method is data efficient, but more computationally expensive, being quadratic in the number of weights.  It can learn quickly, but never forgets, so does not adapt to policy changes.\n",
    "\n",
    "#### Memory based function approximation\n",
    "Memory based methods save training examples as they arrive, and when a state value estimate is needed, they choose a set of examples from which to compute a value estimate.  This is also called lazy learning, since processing is postponed until needed.  Local learning approximate a value based on observations near it, like nearest neighbors.  Locally weighted regression is similar, but makes a parametric approximation of a surface fit to the nearest states.  Local methods can also function well in cases where only a small area of the total space is of interest.\n",
    "\n",
    "#### Kernel based function approximation\n",
    "Local methods depend on assigning weights to examples based on the distance between the state you want to estimate (query state) and the states you have observed.  The function that assigns these weights is called a kernel function, or kernel.  In the weighted average method, the function assigns weights to distances between states.  \n",
    "$$\n",
    "k: \\cal S \\times \\cal S \\to \\bf R\n",
    "$$\n",
    "with the weight being $k(s,s^\\prime)$.  Alternatively, view k as a measure of generalization between states expressing how relevant knowledge about one state is to another.  Kernel regression is a memory based methods computing a kernel weighted average of the targets of all examples D:\n",
    "$$\n",
    "\\hat v = \\sum_{s \\in D} k(s,s^\\prime)g(s)\n",
    "$$\n",
    "where g is the target (observed value) of state s.  The guassian radial basis function (RBF) is one example.  Worth noting is that any linear parametric regression method with feature vectors x can be recast as kernel regression with a kernel that is an inner product of the feature vectors.  This implies that instead of constructing features for linear parametric function approximators, one can construct kernel functions directly without referring to feature vectors, meaning that computation can be done without using the features space at all.  This \"kernel trick\" allows working as if with a high dimension of an expansive features space while actually working only with the training examples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On policy control with approximation\n",
    "Returning to control, we need to estimate state action pairs instead of just state values. The example algorithm is semi-gradient sarsa, which extends TD(0) to action values and control.  The episodic case is easier, so we start with that.  The update target is determined by the state and action, $S_t, A_t \\to U_t$, and can be any approximation to $q_\\pi$, like $G_t$ in MC, or the n step returns.  The gradient-descent based update is\n",
    "$$\n",
    "w_{t+1} = w_t + \\alpha \\left[ U_t - \\hat q(S_t,A_t,w_t) \\right] \\nabla \\hat q(S_t,A_t,w_t)\n",
    "$$\n",
    "For one step sarsa, this is\n",
    "$$\n",
    "w_{t+1} = w_t + \\alpha \\left[ R_{t +1} + \\gamma \\hat q(S_{t+1},A_{t+1},w_t) - \\hat q(S_t,A_t,w_t) \\right] \\nabla \\hat q(S_t,A_t,w_t)\n",
    "$$\n",
    "also known as episodic semi gradient one step sarsa.  To form control methods, iterate sarsa through an episode, choosing actions according to an epsilon greedy policy (for example).\n",
    "\n",
    "#### Average reward\n",
    "When the task is not episodic, an option is to use average rewards, which have no discounting.  In this case, the quality of a policy is determined as the average rate of reward (average reward) while following the policy.\n",
    "$$\n",
    "r(\\pi) = \\sum_s \\mu_\\pi (s) \\sum_a \\pi(a|s) \\sum_{s^\\prime,a} p(s^\\prime, r| s,a)r = \\lim_{h \\to \\infty} \\frac{1}{h} \\sum_{t=1}^h E [ R_t | S_0, A_{a:t-1}\\sim \\pi]\n",
    "$$\n",
    "where mu is the steady state distribution, which is assumed to be independent of the intitial condition (ergodic) in the long term.\n",
    "$$\n",
    "\\mu_\\pi(s) = \\lim_{t \\to \\infty} P(S_t=s |   A_{a:t-1}\\sim \\pi)\n",
    "$$\n",
    "Now, returns are defined as differences between rewards and the average reward:\n",
    "$$\n",
    "G_t:=  (R_{t+1}-r(\\pi)) + \\dots + (R_{t+n}-r(\\pi))+ \\dots\n",
    "$$\n",
    "This is the differential return, and the value function corresponding to it is a differential value funtion.  There are bellman equations, with $r-r(\\pi)$ replacing $R_t, \\gamma$.  For bootstrapping, average reward as of time t is represented by $\\bar R_t$, and the TD errors are \n",
    "$$\n",
    "\\delta_t=R_{t+1}-\\bar R_t+ \\hat q (S_{t+1},A_{t+1},w_t)- \\hat q (S_{t},A_{t},w_t)\n",
    "$$\n",
    "so that (average reward) differential semi gradient sarsa is updated by \n",
    "$$\n",
    "w_{t+1} = w_t+\\alpha \\delta_t \\nabla \\hat q (S_{t},A_{t},w_t)\n",
    "$$\n",
    "\n",
    "Sutton & Bardo note that in the approximate case, and especially cases with an infinite sequence of rewards or without clearly defined states, discounting is not generally useful.  One reason is that because we are not updating states individually, we do not know that improved state estimates improve the policy, they more likely make some states better and others worse, so we have lost a guarantee of convergence to an optimal policy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Off policy with approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eligibility Traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Gradient methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
