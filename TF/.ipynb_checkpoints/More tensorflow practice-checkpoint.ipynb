{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df55cc47",
   "metadata": {},
   "source": [
    "Imperial college week 3\n",
    "callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2750f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f39f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can define your own callbacks by  overwriting the methods \n",
    "class LossAndMetricCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    # Print the loss after every second batch in the training set\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch %2 ==0:\n",
    "            print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
    "    \n",
    "    # Print the loss after each batch in the test set\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n",
    "\n",
    "    # Print the loss and mean absolute error after each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n",
    "    \n",
    "    # Notify the user when prediction has finished on each batch\n",
    "    def on_predict_batch_end(self,batch, logs=None):\n",
    "        print(\"Finished prediction on batch {}!\".format(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c197f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "history = model.fit(train_data, train_targets, epochs=20,\n",
    "                    batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3067ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#or change a parameter by epoc, batch etc\n",
    "# Define the custom callback\n",
    "\n",
    "class LRScheduler(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, new_lr):\n",
    "        super(LRScheduler, self).__init__()\n",
    "        # Add the new learning rate function to our callback\n",
    "        self.new_lr = new_lr\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n",
    "        if not hasattr(self.model.optimizer, 'lr'):\n",
    "              raise ValueError('Error: Optimizer does not have a learning rate.')\n",
    "                \n",
    "        # Get the current learning rate\n",
    "        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n",
    "        \n",
    "        # Call the auxillary function to get the scheduled learning rate for the current epoch\n",
    "        scheduled_rate = self.new_lr(epoch, curr_rate)\n",
    "\n",
    "        # Set the learning rate to the scheduled learning rate\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n",
    "        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))\n",
    "lr_schedule = [\n",
    "    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n",
    "]\n",
    "\n",
    "def get_new_epoch_lr(epoch, lr):\n",
    "    # Checks to see if the input epoch is listed in the learning rate schedule \n",
    "    # and if so, returns index in lr_schedule\n",
    "    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0]==int(epoch)]\n",
    "    if len(epoch_in_sched)>0:\n",
    "        # If it is, return the learning rate corresponding to the epoch\n",
    "        return lr_schedule[epoch_in_sched[0]][1]\n",
    "    else:\n",
    "        # Otherwise, return the existing learning rate\n",
    "        return lr\n",
    "\n",
    "#use as \n",
    "\n",
    "new_history = new_model.fit(train_data, train_targets, epochs=20,\n",
    "                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)\n",
    "https://www.tensorflow.org/guide/keras/custom_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e65ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras model to add callbacks to\n",
    "def get_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(1, input_dim=784))\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\"mean_absolute_error\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "621b4707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Load example MNIST data and pre-process it\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "\n",
    "# Limit the data to 1000 samples\n",
    "x_train = x_train[:1000]\n",
    "y_train = y_train[:1000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17522769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 13:01:21.803987: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-05-24 13:01:21.804182: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-24 13:01:21.804930: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-05-24 13:01:21.857331: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-05-24 13:01:21.876105: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2793530000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up to batch 0, the average loss is   29.87.\n",
      "Up to batch 1, the average loss is  423.68.\n",
      "Up to batch 2, the average loss is  290.05.\n",
      "Up to batch 3, the average loss is  220.50.\n",
      "Up to batch 4, the average loss is  178.31.\n",
      "The average loss for epoch 0 is  178.31 and mean absolute error is    8.00.\n",
      "Up to batch 0, the average loss is    4.93.\n",
      "Up to batch 1, the average loss is    5.30.\n",
      "Up to batch 2, the average loss is    5.58.\n",
      "Up to batch 3, the average loss is    5.25.\n",
      "Up to batch 4, the average loss is    4.97.\n",
      "The average loss for epoch 1 is    4.97 and mean absolute error is    1.85.\n",
      "Up to batch 0, the average loss is    4.18.\n",
      "Up to batch 1, the average loss is    5.05.\n",
      "Up to batch 2, the average loss is    5.91.\n",
      "Up to batch 3, the average loss is    6.79.\n",
      "Up to batch 4, the average loss is    8.17.\n",
      "The average loss for epoch 2 is    8.17 and mean absolute error is    2.35.\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f57068487f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()\n",
    "        self.patience = patience\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        current = logs.get(\"loss\")\n",
    "        if np.less(current, self.best):\n",
    "            self.best = current\n",
    "            self.wait = 0\n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True\n",
    "                print(\"Restoring model weights from the end of the best epoch.\")\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epoch %05d: early stopping\" % (self.stopped_epoch + 1))\n",
    "\n",
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\n",
    "            \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "        )\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\n",
    "            \"Up to batch {}, the average loss is {:7.2f}.\".format(batch, logs[\"loss\"])\n",
    "        )\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \"\n",
    "            \"and mean absolute error is {:7.2f}.\".format(\n",
    "                epoch, logs[\"loss\"], logs[\"mean_absolute_error\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "model = get_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=5,\n",
    "    epochs=30,\n",
    "    verbose=0,\n",
    "    callbacks=[LossAndErrorPrintingCallback(), EarlyStoppingAtMinLoss()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bcfb9f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m callback \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# This callback will stop the training when there is no improvement in\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the loss for three consecutive epochs.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "# This callback will stop the training when there is no improvement in\n",
    "# the loss for three consecutive epochs.\n",
    "\n",
    "model = get_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=5,\n",
    "    epochs=30,\n",
    "    verbose=0,\n",
    "    callbacks=[callback],\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66f0861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model checkpoint -- can create a checkpoint to save models, weights, architecture, whatever with ModelCheckpoint, \n",
    "#which is used as a callback\n",
    "#can only save best (according to specified criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb47323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/craig/anaconda3/envs/test/bin/tensorboard\", line 10, in <module>\r\n",
      "    sys.exit(run_main())\r\n",
      "  File \"/home/craig/anaconda3/envs/test/lib/python3.9/site-packages/tensorboard/main.py\", line 65, in run_main\r\n",
      "    default.get_plugins(),\r\n",
      "  File \"/home/craig/anaconda3/envs/test/lib/python3.9/site-packages/tensorboard/default.py\", line 108, in get_plugins\r\n",
      "    return get_static_plugins() + get_dynamic_plugins()\r\n",
      "  File \"/home/craig/anaconda3/envs/test/lib/python3.9/site-packages/tensorboard/default.py\", line 143, in get_dynamic_plugins\r\n",
      "    return [\r\n",
      "  File \"/home/craig/anaconda3/envs/test/lib/python3.9/site-packages/tensorboard/default.py\", line 144, in <listcomp>\r\n",
      "    entry_point.load()\r\n",
      "  File \"/home/craig/anaconda3/envs/test/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 2457, in load\r\n",
      "    self.require(*args, **kwargs)\r\n",
      "  File \"/home/craig/anaconda3/envs/test/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 2480, in require\r\n",
      "    items = working_set.resolve(reqs, env, installer, extras=self.extras)\r\n",
      "  File \"/home/craig/anaconda3/envs/test/lib/python3.9/site-packages/pkg_resources/__init__.py\", line 788, in resolve\r\n",
      "    raise VersionConflict(dist, req).with_context(dependent_req)\r\n",
      "pkg_resources.VersionConflict: (google-auth 2.6.0 (/home/craig/anaconda3/envs/test/lib/python3.9/site-packages), Requirement.parse('google-auth<2,>=1.6.3'))\r\n"
     ]
    }
   ],
   "source": [
    "#examine tensorboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
